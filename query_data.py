import argparse
import logging

from langchain_chroma.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama

from get_embedding_function import create_embedding_model

CHROMA_DIRECTORY = "chroma"

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}
If the answer is not present in the context, please say "I don't know".
Give the answer in French.

---

Answer the question based on the above context: {question}
"""


def generate_answer_from_query(user_question: str) -> str:
    """
    Queries the vector database with the user's question and generates an answer
    based on the most relevant documents.

    :param user_question: The question provided by the user.
    :return: The answer generated by the AI, or an error message in case of issues.
    """
    try:
        # Initialize the vector database with the embedding function
        embedding_function = create_embedding_model()
        vector_database = Chroma(
            persist_directory=CHROMA_DIRECTORY,
            embedding_function=embedding_function
        )
    except Exception as error:
        logging.error("Error initializing the vector database: %s", error)
        return "Error initializing the vector database."

    try:
        # Search for the most relevant documents based on the question
        relevant_documents = vector_database.similarity_search_with_score(user_question, k=5)
    except Exception as error:
        logging.error("Error during document similarity search: %s", error)
        return "Error searching for relevant documents."

    # Combine document contents to form the context
    context_text = "\n\n---\n\n".join([doc.page_content for doc, _ in relevant_documents])

    # Prepare the prompt with the context and the question
    prompt_instance = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prepared_prompt = prompt_instance.format(context=context_text, question=user_question)

    try:
        # Call the AI model to generate the answer
        ai_model = Ollama(model="mistral")
        generated_answer = ai_model.invoke(prepared_prompt)
    except Exception as error:
        logging.error("Error calling the AI model: %s", error)
        return "Error generating the answer with the AI."

    # Log the sources of the documents used
    document_sources = [doc.metadata.get("id", "Unknown ID") for doc, _ in relevant_documents]
    logging.info("Sources used: %s", ", ".join(document_sources))

    return generated_answer


def query_rag(user_question: str) -> str:
    """
    Alias for generate_answer_from_query to be used externally.

    :param user_question: The question provided by the user.
    :return: The generated answer.
    """
    return generate_answer_from_query(user_question)


def main():
    parser = argparse.ArgumentParser(
        description="Query the vector database and generate an AI answer based on a user's question."
    )
    parser.add_argument("user_question", type=str, help="The question to find relevant documents.")
    args = parser.parse_args()

    answer = generate_answer_from_query(args.user_question)
    print(answer)


if __name__ == "__main__":
    main()
